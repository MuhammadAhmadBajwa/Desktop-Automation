# ğŸ§  Desktop Automation Bot

An intelligent desktop assistant that performs real-time automation on Windows using **voice commands** and **visual understanding of the screen** â€” powered by Python, computer vision, and LLMs. No hardcoded scripts. Just intelligent task execution.

---

## ğŸ§  Core Idea

You speak a high-level command like:

> â€œCheck my IoT assignmentâ€

The assistant:

* Understands the command using an LLM (e.g., LLaMA 3.3 hosted via Groq)
* Analyzes the screen visually
* Matches the intent to visible elements
* Clicks, types, and automates step-by-step until the task is complete

---

## ğŸ¤ Voice-Based Interaction

* **Voice Input**: Captured via your microphone using `speech_recognition`
* **Voice Output**: Bot replies and confirms actions using `pyttsx3` (offline text-to-speech)

---

## ğŸ”„ How It Works

1. **ğŸ™ Voice Input:**

   * Your voice is captured using the `speech_recognition` library and converted to text.

2. **ğŸ’¡ Command Understanding:**

   * The transcribed command is sent to an LLM (via Groq API).
   * It returns a list of subtasks such as:

     * â€œOpen Chromeâ€
     * â€œGo to Google Classroomâ€
     * â€œClick on IoT assignmentâ€

3. **ğŸ–¥ï¸ Visual Detection:**

   * A screenshot of the desktop is taken using PyAutoGUI or MSS.
   * YOLO detects all clickable UI elements (icons, buttons, etc.).
   * Each detected item is passed to Florence (770M parameter model) to generate a descriptive caption.

4. **ğŸ”— Matching Tasks to UI:**

   * The LLM receives the list of subtasks along with the detected UI items (descriptions + IDs).
   * It selects the most relevant clickable element for each subtask.

5. **âš™ï¸ UI Automation:**

   * PyAutoGUI simulates mouse clicks, keystrokes, or scroll actions.
   * The assistant loops back to the desktop and moves to the next subtask.

---

## âš™ï¸ Technologies Used

| Component             | Tool/Library           |
| --------------------- | ---------------------- |
| Voice Input           | `speech_recognition`   |
| Text-to-Speech Output | `pyttsx3`              |
| Screen Capture        | `PyAutoGUI`            |
| Object Detection      | `YOLO`                 |
| Image Captioning      | `Florence` (770M)      |
| Language Model (LLM)  | `LLaMA` (via Groq API) |
| Automation Execution  | `PyAutoGUI`            |
| Icon Caching\Retrival | `FAISS`  `RESNET`      |
| Language              | Python                 |

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/MuhammadAhmadBajwa/Desktop-Automation.git
cd Desktop-Automation
```

### 2. Create and Activate Virtual Environment

```bash
python -m venv venv
```

Activate it:

* **Windows**:

  ```bash
  venv\Scripts\activate
  ```
* **macOS/Linux**:

  ```bash
  source venv/bin/activate
  ```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Run the Assistant

```bash
python app.py
```

---


---

## âš ï¸ Challenges

* â³ **Captioning is slow** on CPU (\~10s/icon) when encountering new UI elements
* ğŸ”² **YOLO may produce inaccurate bounding boxes** in cluttered or dense UI layouts
* ğŸ§¾ **Florence model can miscaption rare or ambiguous UI elements**
* ğŸ“¸ **No existing dataset** for UI component captioning â€” requires manual labeling for training
* ğŸ§  **Solution:** Added a **vector searchâ€“based caching mechanism** using **ResNet image embeddings**
  â†’ As the assistant is used more often, most common icons/UI elements are **cached**, reducing captioning time dramatically on repeated use
---


---

## ğŸ¯ Project Goal

To build a **real-time, intelligent automation system** that adapts to desktop environments, understands **natural language**, and performs tasks without needing static scripts.



